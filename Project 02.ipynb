{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d3c07ee-c3bf-4530-812a-36405502b38d",
   "metadata": {
    "id": "9d3c07ee-c3bf-4530-812a-36405502b38d"
   },
   "source": [
    "# Home Credit Default Risk\n",
    "\n",
    "\n",
    "Pipeline to preprocess the data, train your model and then predict values for the [Home Credit Default Risk](https://www.kaggle.com/competitions/home-credit-default-risk/) Kaggle competition.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e396c950-04b0-453e-b930-a22a96cee2d1",
   "metadata": {
    "id": "e396c950-04b0-453e-b930-a22a96cee2d1"
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This is a binary Classification task: we want to predict whether the person applying for a home credit will be able to repay their debt or not. Our model will have to predict a 1 indicating the client will have payment difficulties: he/she will have late payment of more than X days on at least one of the first Y installments of the loan in our sample, 0 in all other cases.\n",
    "\n",
    "The dataset is composed of multiple files with different information about loans taken. In this project, we will work exclusively with the primary files: `application_train_aai.csv` and `application_test_aai.csv`.\n",
    "\n",
    "We will use [Area Under the ROC Curve](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc?hl=es_419) as the evaluation metric, so our models will have to return the probabilities that a loan is not paid for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "OzQjTwlkUT0C",
   "metadata": {
    "executionInfo": {
     "elapsed": 2252,
     "status": "ok",
     "timestamp": 1670194396248,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "OzQjTwlkUT0C"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src import config, data_utils, preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ab085b5-379c-4e02-9f96-08edf5dbf887",
   "metadata": {
    "id": "3ab085b5-379c-4e02-9f96-08edf5dbf887"
   },
   "source": [
    "### Getting the data\n",
    "\n",
    "To access the data for this project, you only need to execute the code below. This will download three files inside the `dataset` folder:\n",
    "\n",
    "- `application_train_aai.csv`: Training dataset you must use to train and find the best hyperparameters on your model.\n",
    "\n",
    "- `application_test_aai.csv`: Test dataset without labels. Because of the absence of labels, you can't use this dataset for your experiments. You will use the file only at the end after you choose what you think is the best model for the tasks. You will have to use that model to fill values in the `TARGET` column using the model predictions. Then submit this dataset alongside this Jupyter notebook, AnyoneAI will internally evaluate your model's accuracy in the hidden data and communicate later ;).\n",
    "\n",
    "- `HomeCredit_columns_description.csv`: This file contains descriptions for the columns in train and test datasets.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25a0a724-ceb1-40cb-b123-b8c907a9c06f",
   "metadata": {
    "id": "25a0a724-ceb1-40cb-b123-b8c907a9c06f"
   },
   "source": [
    "1.1. Load the training and test datasets. Also, the auxiliary file `HomeCredit_columns_description.csv` has additional information about the features in the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "MnA4l8-rWraC",
   "metadata": {
    "id": "MnA4l8-rWraC"
   },
   "source": [
    "**Don't change anything in this cell, just make it run correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "jrkLdOJnWoSS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1670195316027,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "jrkLdOJnWoSS",
    "outputId": "ce9f5ee1-6ed0-4b6f-d8f5-37d38b4e4773"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: app_train shape is correct!\n",
      "Success: app_train type is correct!\n",
      "Success: app_test shape is correct!\n",
      "Success: app_test type is correct!\n"
     ]
    }
   ],
   "source": [
    "app_train, app_test, columns_description = data_utils.get_datasets()\n",
    "\n",
    "\n",
    "if app_train.shape == (246008, 122):\n",
    "    print(\"Success: app_train shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Train dataset shape is incorrect, please review your code\")\n",
    "\n",
    "if isinstance(app_train, pd.DataFrame):\n",
    "    print(\"Success: app_train type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Train dataset type is incorrect, please review your code\")\n",
    "\n",
    "if app_test.shape == (61503, 122):\n",
    "    print(\"Success: app_test shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Test dataset shape is incorrect, please review your code\")\n",
    "\n",
    "if isinstance(app_test, pd.DataFrame):\n",
    "    print(\"Success: app_test type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Test dataset type is incorrect, please review your code\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c10a608-5c38-44f4-8158-18225619e7ae",
   "metadata": {
    "id": "7c10a608-5c38-44f4-8158-18225619e7ae",
    "tags": []
   },
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "A lot of the analysis of the data can be found on publicly available Kaggle kernels or blog posts, but you need to make sure you understand the dataset's properties before starting working on it, so we'll do exploratory data analysis for the main files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ea774d3-e12c-4c2b-846d-8c5c03e70928",
   "metadata": {
    "id": "2ea774d3-e12c-4c2b-846d-8c5c03e70928"
   },
   "source": [
    "#### Dataset Basics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f0b4817-5768-49d2-b5f2-907827541f16",
   "metadata": {
    "id": "9f0b4817-5768-49d2-b5f2-907827541f16"
   },
   "source": [
    "1.2. Print how many samples do we have in our train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a98fdd5-cbf2-4d20-9559-89c7cf5943cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1670195319978,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "6a98fdd5-cbf2-4d20-9559-89c7cf5943cd",
    "outputId": "bec830ab-d760-4019-d277-d4249be2948d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61503, 122)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO Complete in this cell: shape of the dataset\n",
    "app_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74e7a95b-8288-4ead-8c1e-f2cf68167d8e",
   "metadata": {
    "id": "74e7a95b-8288-4ead-8c1e-f2cf68167d8e"
   },
   "source": [
    "1.3. List all columns in the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7af2f2e-93db-41e6-bb26-df1ad0be7786",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1670195328520,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "d7af2f2e-93db-41e6-bb26-df1ad0be7786",
    "outputId": "cc0cbeb7-edf1-4d3a-cb05-3341232c2606"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SK_ID_CURR',\n",
       " 'TARGET',\n",
       " 'NAME_CONTRACT_TYPE',\n",
       " 'CODE_GENDER',\n",
       " 'FLAG_OWN_CAR',\n",
       " 'FLAG_OWN_REALTY',\n",
       " 'CNT_CHILDREN',\n",
       " 'AMT_INCOME_TOTAL',\n",
       " 'AMT_CREDIT',\n",
       " 'AMT_ANNUITY',\n",
       " 'AMT_GOODS_PRICE',\n",
       " 'NAME_TYPE_SUITE',\n",
       " 'NAME_INCOME_TYPE',\n",
       " 'NAME_EDUCATION_TYPE',\n",
       " 'NAME_FAMILY_STATUS',\n",
       " 'NAME_HOUSING_TYPE',\n",
       " 'REGION_POPULATION_RELATIVE',\n",
       " 'DAYS_BIRTH',\n",
       " 'DAYS_EMPLOYED',\n",
       " 'DAYS_REGISTRATION',\n",
       " 'DAYS_ID_PUBLISH',\n",
       " 'OWN_CAR_AGE',\n",
       " 'FLAG_MOBIL',\n",
       " 'FLAG_EMP_PHONE',\n",
       " 'FLAG_WORK_PHONE',\n",
       " 'FLAG_CONT_MOBILE',\n",
       " 'FLAG_PHONE',\n",
       " 'FLAG_EMAIL',\n",
       " 'OCCUPATION_TYPE',\n",
       " 'CNT_FAM_MEMBERS',\n",
       " 'REGION_RATING_CLIENT',\n",
       " 'REGION_RATING_CLIENT_W_CITY',\n",
       " 'WEEKDAY_APPR_PROCESS_START',\n",
       " 'HOUR_APPR_PROCESS_START',\n",
       " 'REG_REGION_NOT_LIVE_REGION',\n",
       " 'REG_REGION_NOT_WORK_REGION',\n",
       " 'LIVE_REGION_NOT_WORK_REGION',\n",
       " 'REG_CITY_NOT_LIVE_CITY',\n",
       " 'REG_CITY_NOT_WORK_CITY',\n",
       " 'LIVE_CITY_NOT_WORK_CITY',\n",
       " 'ORGANIZATION_TYPE',\n",
       " 'EXT_SOURCE_1',\n",
       " 'EXT_SOURCE_2',\n",
       " 'EXT_SOURCE_3',\n",
       " 'APARTMENTS_AVG',\n",
       " 'BASEMENTAREA_AVG',\n",
       " 'YEARS_BEGINEXPLUATATION_AVG',\n",
       " 'YEARS_BUILD_AVG',\n",
       " 'COMMONAREA_AVG',\n",
       " 'ELEVATORS_AVG',\n",
       " 'ENTRANCES_AVG',\n",
       " 'FLOORSMAX_AVG',\n",
       " 'FLOORSMIN_AVG',\n",
       " 'LANDAREA_AVG',\n",
       " 'LIVINGAPARTMENTS_AVG',\n",
       " 'LIVINGAREA_AVG',\n",
       " 'NONLIVINGAPARTMENTS_AVG',\n",
       " 'NONLIVINGAREA_AVG',\n",
       " 'APARTMENTS_MODE',\n",
       " 'BASEMENTAREA_MODE',\n",
       " 'YEARS_BEGINEXPLUATATION_MODE',\n",
       " 'YEARS_BUILD_MODE',\n",
       " 'COMMONAREA_MODE',\n",
       " 'ELEVATORS_MODE',\n",
       " 'ENTRANCES_MODE',\n",
       " 'FLOORSMAX_MODE',\n",
       " 'FLOORSMIN_MODE',\n",
       " 'LANDAREA_MODE',\n",
       " 'LIVINGAPARTMENTS_MODE',\n",
       " 'LIVINGAREA_MODE',\n",
       " 'NONLIVINGAPARTMENTS_MODE',\n",
       " 'NONLIVINGAREA_MODE',\n",
       " 'APARTMENTS_MEDI',\n",
       " 'BASEMENTAREA_MEDI',\n",
       " 'YEARS_BEGINEXPLUATATION_MEDI',\n",
       " 'YEARS_BUILD_MEDI',\n",
       " 'COMMONAREA_MEDI',\n",
       " 'ELEVATORS_MEDI',\n",
       " 'ENTRANCES_MEDI',\n",
       " 'FLOORSMAX_MEDI',\n",
       " 'FLOORSMIN_MEDI',\n",
       " 'LANDAREA_MEDI',\n",
       " 'LIVINGAPARTMENTS_MEDI',\n",
       " 'LIVINGAREA_MEDI',\n",
       " 'NONLIVINGAPARTMENTS_MEDI',\n",
       " 'NONLIVINGAREA_MEDI',\n",
       " 'FONDKAPREMONT_MODE',\n",
       " 'HOUSETYPE_MODE',\n",
       " 'TOTALAREA_MODE',\n",
       " 'WALLSMATERIAL_MODE',\n",
       " 'EMERGENCYSTATE_MODE',\n",
       " 'OBS_30_CNT_SOCIAL_CIRCLE',\n",
       " 'DEF_30_CNT_SOCIAL_CIRCLE',\n",
       " 'OBS_60_CNT_SOCIAL_CIRCLE',\n",
       " 'DEF_60_CNT_SOCIAL_CIRCLE',\n",
       " 'DAYS_LAST_PHONE_CHANGE',\n",
       " 'FLAG_DOCUMENT_2',\n",
       " 'FLAG_DOCUMENT_3',\n",
       " 'FLAG_DOCUMENT_4',\n",
       " 'FLAG_DOCUMENT_5',\n",
       " 'FLAG_DOCUMENT_6',\n",
       " 'FLAG_DOCUMENT_7',\n",
       " 'FLAG_DOCUMENT_8',\n",
       " 'FLAG_DOCUMENT_9',\n",
       " 'FLAG_DOCUMENT_10',\n",
       " 'FLAG_DOCUMENT_11',\n",
       " 'FLAG_DOCUMENT_12',\n",
       " 'FLAG_DOCUMENT_13',\n",
       " 'FLAG_DOCUMENT_14',\n",
       " 'FLAG_DOCUMENT_15',\n",
       " 'FLAG_DOCUMENT_16',\n",
       " 'FLAG_DOCUMENT_17',\n",
       " 'FLAG_DOCUMENT_18',\n",
       " 'FLAG_DOCUMENT_19',\n",
       " 'FLAG_DOCUMENT_20',\n",
       " 'FLAG_DOCUMENT_21',\n",
       " 'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
       " 'AMT_REQ_CREDIT_BUREAU_DAY',\n",
       " 'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
       " 'AMT_REQ_CREDIT_BUREAU_MON',\n",
       " 'AMT_REQ_CREDIT_BUREAU_QRT',\n",
       " 'AMT_REQ_CREDIT_BUREAU_YEAR']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO Complete in this cell: Show all columns in the training dataset\n",
    "list(app_test.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2dc4835f-90c3-4049-9ec2-44d102201104",
   "metadata": {
    "id": "2dc4835f-90c3-4049-9ec2-44d102201104"
   },
   "source": [
    "1.4. Show the first 5 records of the training dataset, transpose the dataframe to see each record as a column and features as rows, make sure all features are visualized. Take your time to review what kind of information you can gather from this data.\n",
    "\n",
    "For reference only, it should look like this:\n",
    "\n",
    "|0|1|2|3|4\n",
    "|---|---|---|---|---\n",
    "Unnamed: 0|187399|84777|268140|270686|33785\n",
    "SK_ID_CURR|317244|198357|410700|413785|139141\n",
    "TARGET|0|0|0|0|0\n",
    "NAME_CONTRACT_TYPE|Cash loans|Cash loans|Cash loans|Cash loans|Cash loans\n",
    "...|...|...|...|...|...\n",
    "AMT_REQ_CREDIT_BUREAU_DAY|0.0|0.0|0.0|0.0|0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac9c85b3-52e0-4b80-9753-afb81f92bd0d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1670195332530,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "ac9c85b3-52e0-4b80-9753-afb81f92bd0d",
    "outputId": "2d56774f-18a2-48f2-f4b4-da9decca6012"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAME_INCOME_TYPE</th>\n",
       "      <td>Pensioner</td>\n",
       "      <td>Working</td>\n",
       "      <td>Commercial associate</td>\n",
       "      <td>Working</td>\n",
       "      <td>Working</td>\n",
       "      <td>State servant</td>\n",
       "      <td>Working</td>\n",
       "      <td>Working</td>\n",
       "      <td>Working</td>\n",
       "      <td>Working</td>\n",
       "      <td>Commercial associate</td>\n",
       "      <td>Commercial associate</td>\n",
       "      <td>Working</td>\n",
       "      <td>Commercial associate</td>\n",
       "      <td>Pensioner</td>\n",
       "      <td>Pensioner</td>\n",
       "      <td>Commercial associate</td>\n",
       "      <td>Working</td>\n",
       "      <td>Commercial associate</td>\n",
       "      <td>Working</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAME_EDUCATION_TYPE</th>\n",
       "      <td>Higher education</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0                 1                     2   \\\n",
       "FLAG_OWN_CAR                        N                 Y                     N   \n",
       "FLAG_OWN_REALTY                     N                 Y                     Y   \n",
       "NAME_INCOME_TYPE            Pensioner           Working  Commercial associate   \n",
       "NAME_EDUCATION_TYPE  Higher education  Higher education      Higher education   \n",
       "\n",
       "                                                3   \\\n",
       "FLAG_OWN_CAR                                     N   \n",
       "FLAG_OWN_REALTY                                  N   \n",
       "NAME_INCOME_TYPE                           Working   \n",
       "NAME_EDUCATION_TYPE  Secondary / secondary special   \n",
       "\n",
       "                                                4   \\\n",
       "FLAG_OWN_CAR                                     N   \n",
       "FLAG_OWN_REALTY                                  N   \n",
       "NAME_INCOME_TYPE                           Working   \n",
       "NAME_EDUCATION_TYPE  Secondary / secondary special   \n",
       "\n",
       "                                                5                 6   \\\n",
       "FLAG_OWN_CAR                                     N                 N   \n",
       "FLAG_OWN_REALTY                                  Y                 N   \n",
       "NAME_INCOME_TYPE                     State servant           Working   \n",
       "NAME_EDUCATION_TYPE  Secondary / secondary special  Higher education   \n",
       "\n",
       "                                                7                 8   \\\n",
       "FLAG_OWN_CAR                                     N                 N   \n",
       "FLAG_OWN_REALTY                                  Y                 Y   \n",
       "NAME_INCOME_TYPE                           Working           Working   \n",
       "NAME_EDUCATION_TYPE  Secondary / secondary special  Higher education   \n",
       "\n",
       "                                                9                     10  \\\n",
       "FLAG_OWN_CAR                                     N                     N   \n",
       "FLAG_OWN_REALTY                                  Y                     Y   \n",
       "NAME_INCOME_TYPE                           Working  Commercial associate   \n",
       "NAME_EDUCATION_TYPE  Secondary / secondary special      Higher education   \n",
       "\n",
       "                                                11  \\\n",
       "FLAG_OWN_CAR                                     N   \n",
       "FLAG_OWN_REALTY                                  Y   \n",
       "NAME_INCOME_TYPE              Commercial associate   \n",
       "NAME_EDUCATION_TYPE  Secondary / secondary special   \n",
       "\n",
       "                                                12                    13  \\\n",
       "FLAG_OWN_CAR                                     Y                     Y   \n",
       "FLAG_OWN_REALTY                                  Y                     N   \n",
       "NAME_INCOME_TYPE                           Working  Commercial associate   \n",
       "NAME_EDUCATION_TYPE  Secondary / secondary special      Higher education   \n",
       "\n",
       "                                                14  \\\n",
       "FLAG_OWN_CAR                                     N   \n",
       "FLAG_OWN_REALTY                                  Y   \n",
       "NAME_INCOME_TYPE                         Pensioner   \n",
       "NAME_EDUCATION_TYPE  Secondary / secondary special   \n",
       "\n",
       "                                                15                    16  \\\n",
       "FLAG_OWN_CAR                                     N                     N   \n",
       "FLAG_OWN_REALTY                                  Y                     Y   \n",
       "NAME_INCOME_TYPE                         Pensioner  Commercial associate   \n",
       "NAME_EDUCATION_TYPE  Secondary / secondary special      Higher education   \n",
       "\n",
       "                                                17                    18  \\\n",
       "FLAG_OWN_CAR                                     N                     Y   \n",
       "FLAG_OWN_REALTY                                  Y                     Y   \n",
       "NAME_INCOME_TYPE                           Working  Commercial associate   \n",
       "NAME_EDUCATION_TYPE  Secondary / secondary special      Higher education   \n",
       "\n",
       "                                                19  \n",
       "FLAG_OWN_CAR                                     N  \n",
       "FLAG_OWN_REALTY                                  N  \n",
       "NAME_INCOME_TYPE                           Working  \n",
       "NAME_EDUCATION_TYPE  Secondary / secondary special  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO Complete in this cell: show first 5 records in a transposed table\n",
    "app_train[[\"FLAG_OWN_CAR\", \"FLAG_OWN_REALTY\", \"NAME_INCOME_TYPE\", \"NAME_EDUCATION_TYPE\"]].head(20).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59da8f46-e503-4b67-8e73-ac61c394824c",
   "metadata": {
    "id": "59da8f46-e503-4b67-8e73-ac61c394824c"
   },
   "source": [
    "1.5. Show the distribution of the target variable values: print the total value count and the percentage of each value, plot this relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8855d370-c825-415b-9dd0-9dbdd576fada",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "executionInfo": {
     "elapsed": 637,
     "status": "ok",
     "timestamp": 1670195337134,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "8855d370-c825-415b-9dd0-9dbdd576fada",
    "outputId": "13794aaf-d179-4506-f087-ad8cd0be3e98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    226257\n",
       "1     19751\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO Complete in this cell: show distribution of target variable\n",
    "app_train['TARGET'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc7b5a63-da31-4a61-9e91-9708dc7633a7",
   "metadata": {
    "id": "cc7b5a63-da31-4a61-9e91-9708dc7633a7"
   },
   "source": [
    "1.6. Show the number of columns of each data type.\n",
    "\n",
    "Just for giving you an idea, the output should look like this (not exactly the same numbers):\n",
    "\n",
    "```python\n",
    "float64    45\n",
    "int64      32\n",
    "object     10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b74878dd-cc48-4e69-bc35-e90457d54b3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1666034867408,
     "user": {
      "displayName": "Federico Morales",
      "userId": "06983145799989655383"
     },
     "user_tz": 180
    },
    "id": "b74878dd-cc48-4e69-bc35-e90457d54b3a",
    "outputId": "5c188262-b9b1-4368-b952-fd2b5608a864"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float64    65\n",
       "int64      41\n",
       "object     16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO Complete in this cell: show number of columns per data type\n",
    "app_train.dtypes.value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0a8799e-d32a-4239-a85d-6ff29ab4682f",
   "metadata": {
    "id": "c0a8799e-d32a-4239-a85d-6ff29ab4682f"
   },
   "source": [
    "1.7. For categorical variables (`object` data type), show the number of distinct values in each column (number of labels).\n",
    "\n",
    "Just for giving you an idea, the output should look like this (not exactly the same numbers):\n",
    "\n",
    "```python\n",
    "NAME_CONTRACT_TYPE             5\n",
    "CODE_GENDER                    2\n",
    "FLAG_OWN_CAR                   1\n",
    "FLAG_OWN_REALTY                1\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b294976-dc0d-44bd-9bf6-29ba1f6a2e2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1666034867708,
     "user": {
      "displayName": "Federico Morales",
      "userId": "06983145799989655383"
     },
     "user_tz": 180
    },
    "id": "8b294976-dc0d-44bd-9bf6-29ba1f6a2e2b",
    "outputId": "277be5a9-e286-4d82-956d-7af49e6112ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAME_CONTRACT_TYPE             2\n",
       "CODE_GENDER                    3\n",
       "FLAG_OWN_CAR                   2\n",
       "FLAG_OWN_REALTY                2\n",
       "NAME_TYPE_SUITE                7\n",
       "NAME_INCOME_TYPE               8\n",
       "NAME_EDUCATION_TYPE            5\n",
       "NAME_FAMILY_STATUS             6\n",
       "NAME_HOUSING_TYPE              6\n",
       "OCCUPATION_TYPE               18\n",
       "WEEKDAY_APPR_PROCESS_START     7\n",
       "ORGANIZATION_TYPE             58\n",
       "FONDKAPREMONT_MODE             4\n",
       "HOUSETYPE_MODE                 3\n",
       "WALLSMATERIAL_MODE             7\n",
       "EMERGENCYSTATE_MODE            2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO Complete in this cell: show number of unique values per categorical column\n",
    "app_train_objects = app_test.select_dtypes('object')\n",
    "app_train_objects.nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "967d1938-e27d-4fbd-9bdc-f26364f5fdfd",
   "metadata": {
    "id": "967d1938-e27d-4fbd-9bdc-f26364f5fdfd"
   },
   "source": [
    "1.8. Analyzing missing data: show the percentage of missing data for each column ordered by percentage descending (show only the 20 columns with higher missing pct)\n",
    "\n",
    "Just for giving you an idea, the output should look like this (not exactly the same numbers and columns names):\n",
    "\n",
    "```python\n",
    "                   Total   Percent\n",
    "COMMONAREA_AVG    121000      85.2\n",
    "COMMONAREA_MODE   121000      76.6\n",
    "COMMONAREA_MEDI   121000      62.9\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b65aa7e-d8a2-44b0-9803-ca5277471470",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "executionInfo": {
     "elapsed": 604,
     "status": "ok",
     "timestamp": 1666034868311,
     "user": {
      "displayName": "Federico Morales",
      "userId": "06983145799989655383"
     },
     "user_tz": 180
    },
    "id": "0b65aa7e-d8a2-44b0-9803-ca5277471470",
    "outputId": "dca74fc2-8fd7-4d29-ef67-f58c3abf2e09",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>COMMONAREA_MEDI</th>\n",
       "      <td>172189</td>\n",
       "      <td>69.993252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COMMONAREA_AVG</th>\n",
       "      <td>172189</td>\n",
       "      <td>69.993252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COMMONAREA_MODE</th>\n",
       "      <td>172189</td>\n",
       "      <td>69.993252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NONLIVINGAPARTMENTS_MODE</th>\n",
       "      <td>171096</td>\n",
       "      <td>69.548958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NONLIVINGAPARTMENTS_AVG</th>\n",
       "      <td>171096</td>\n",
       "      <td>69.548958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NONLIVINGAPARTMENTS_MEDI</th>\n",
       "      <td>171096</td>\n",
       "      <td>69.548958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FONDKAPREMONT_MODE</th>\n",
       "      <td>168561</td>\n",
       "      <td>68.518503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LIVINGAPARTMENTS_MODE</th>\n",
       "      <td>168494</td>\n",
       "      <td>68.491269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LIVINGAPARTMENTS_AVG</th>\n",
       "      <td>168494</td>\n",
       "      <td>68.491269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LIVINGAPARTMENTS_MEDI</th>\n",
       "      <td>168494</td>\n",
       "      <td>68.491269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLOORSMIN_AVG</th>\n",
       "      <td>167255</td>\n",
       "      <td>67.987626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLOORSMIN_MODE</th>\n",
       "      <td>167255</td>\n",
       "      <td>67.987626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLOORSMIN_MEDI</th>\n",
       "      <td>167255</td>\n",
       "      <td>67.987626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEARS_BUILD_MEDI</th>\n",
       "      <td>163980</td>\n",
       "      <td>66.656369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEARS_BUILD_MODE</th>\n",
       "      <td>163980</td>\n",
       "      <td>66.656369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEARS_BUILD_AVG</th>\n",
       "      <td>163980</td>\n",
       "      <td>66.656369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OWN_CAR_AGE</th>\n",
       "      <td>162503</td>\n",
       "      <td>66.055982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LANDAREA_MEDI</th>\n",
       "      <td>146436</td>\n",
       "      <td>59.524893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LANDAREA_MODE</th>\n",
       "      <td>146436</td>\n",
       "      <td>59.524893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LANDAREA_AVG</th>\n",
       "      <td>146436</td>\n",
       "      <td>59.524893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Total    Percent\n",
       "COMMONAREA_MEDI           172189  69.993252\n",
       "COMMONAREA_AVG            172189  69.993252\n",
       "COMMONAREA_MODE           172189  69.993252\n",
       "NONLIVINGAPARTMENTS_MODE  171096  69.548958\n",
       "NONLIVINGAPARTMENTS_AVG   171096  69.548958\n",
       "NONLIVINGAPARTMENTS_MEDI  171096  69.548958\n",
       "FONDKAPREMONT_MODE        168561  68.518503\n",
       "LIVINGAPARTMENTS_MODE     168494  68.491269\n",
       "LIVINGAPARTMENTS_AVG      168494  68.491269\n",
       "LIVINGAPARTMENTS_MEDI     168494  68.491269\n",
       "FLOORSMIN_AVG             167255  67.987626\n",
       "FLOORSMIN_MODE            167255  67.987626\n",
       "FLOORSMIN_MEDI            167255  67.987626\n",
       "YEARS_BUILD_MEDI          163980  66.656369\n",
       "YEARS_BUILD_MODE          163980  66.656369\n",
       "YEARS_BUILD_AVG           163980  66.656369\n",
       "OWN_CAR_AGE               162503  66.055982\n",
       "LANDAREA_MEDI             146436  59.524893\n",
       "LANDAREA_MODE             146436  59.524893\n",
       "LANDAREA_AVG              146436  59.524893"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO Complete in this cell: checking missing data\n",
    "percent_missing = app_train.isnull().sum() * 100 / len(app_train)\n",
    "missing_values = pd.DataFrame({'Total': app_train.isnull().sum()\n",
    "                               ,'Percent': percent_missing})\n",
    "\n",
    "missing_values.sort_values(by=['Percent'], ascending=False).head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "783c5bb6-6b6d-4e34-bbe2-3a5dc319a4f0",
   "metadata": {
    "id": "783c5bb6-6b6d-4e34-bbe2-3a5dc319a4f0"
   },
   "source": [
    "#### Analyzing distribution of variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c79d319-bde8-47a4-890d-6694edf221e2",
   "metadata": {
    "id": "9c79d319-bde8-47a4-890d-6694edf221e2"
   },
   "source": [
    "1.9. Show the distribution of credit amounts.\n",
    "\n",
    "*Hint:* Take a look at `AMT_CREDIT` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc953042-9744-498e-a435-fa660e76c70d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "executionInfo": {
     "elapsed": 2137,
     "status": "ok",
     "timestamp": 1666034870446,
     "user": {
      "displayName": "Federico Morales",
      "userId": "06983145799989655383"
     },
     "user_tz": 180
    },
    "id": "bc953042-9744-498e-a435-fa660e76c70d",
    "outputId": "28e9ab9e-788b-4ca8-e796-a672103d6045",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### TODO Complete in this cell: distribution of credit amounts\n",
    "app_train['AMT_CREDIT'].hist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4e7bf4a-25c7-47c0-aea8-ed916d8827f6",
   "metadata": {
    "id": "f4e7bf4a-25c7-47c0-aea8-ed916d8827f6"
   },
   "source": [
    "1.10. Plot the education level of the credit applicants, show the percentages of each category. Also print the total counts for each category.\n",
    "\n",
    "*Hint:* Take a look at `NAME_EDUCATION_TYPE` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "406d7d06-b21a-4938-8e3d-11798def489b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1666034870446,
     "user": {
      "displayName": "Federico Morales",
      "userId": "06983145799989655383"
     },
     "user_tz": 180
    },
    "id": "406d7d06-b21a-4938-8e3d-11798def489b",
    "outputId": "41fa858b-e391-427a-9e2d-b21c97ec0993"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Secondary / secondary special\n",
       "1        Secondary / secondary special\n",
       "2        Secondary / secondary special\n",
       "3                     Higher education\n",
       "4                     Higher education\n",
       "                     ...              \n",
       "61498    Secondary / secondary special\n",
       "61499    Secondary / secondary special\n",
       "61500                 Higher education\n",
       "61501                 Higher education\n",
       "61502    Secondary / secondary special\n",
       "Name: NAME_EDUCATION_TYPE, Length: 61503, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO Complete in this cell: level of education plot\n",
    "app_test['NAME_EDUCATION_TYPE']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "675b1ae3-c943-4737-bdb8-2ecff733b27b",
   "metadata": {
    "id": "675b1ae3-c943-4737-bdb8-2ecff733b27b"
   },
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "VvT_6ZEOztAQ",
   "metadata": {
    "id": "VvT_6ZEOztAQ"
   },
   "source": [
    "2.1. The next step will be to separate our train and test datasets columns between Features (the input to the model) and Targets (what the model has to predict with the given features).\n",
    "\n",
    "- Assign to `X_train` all the columns from `app_train` that should be used as features for training our models.\n",
    "- Assign to `y_train` the single column from `app_train` that should be used as our target (i.e. what we want to predict).\n",
    "- Assign to `X_test` all the columns from `app_test` that should be used as features for training our models.\n",
    "- Assign to `y_test` the single column from `app_test` that should be used as our target (i.e. what we want to predict).\n",
    "\n",
    "To do that, you will have to complete the function `data_utils.get_feature_target()` in all the parts with a `TODO` mark.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "wrkoywq9aNvJ",
   "metadata": {
    "id": "wrkoywq9aNvJ"
   },
   "source": [
    "**Don't change anything in this cell, just make it run correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5F5UeGj1aNvJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 511,
     "status": "ok",
     "timestamp": 1670195363853,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "5F5UeGj1aNvJ",
    "outputId": "6ce8ac80-c09c-43dd-a537-cc7b25220efe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: X_train shape is correct!\n",
      "Success: X_train type is correct!\n",
      "Success: y_train shape is correct!\n",
      "Success: X_test shape is correct!\n",
      "Success: X_test type is correct!\n",
      "Success: y_test shape is correct!\n"
     ]
    }
   ],
   "source": [
    "# Now we execute the function above to get the result\n",
    "X_train, y_train, X_test, y_test = data_utils.get_feature_target(app_train, app_test)\n",
    "\n",
    "\n",
    "if X_train.shape == (246008, 121):\n",
    "    print(\"Success: X_train shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"X_train dataset shape is incorrect, please review your code\")\n",
    "\n",
    "if isinstance(X_train, pd.DataFrame):\n",
    "    print(\"Success: X_train type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Train dataset type is incorrect, please review your code\")\n",
    "\n",
    "if y_train.shape == (246008,) or y_train.shape == (246008, 1):\n",
    "    print(\"Success: y_train shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Train labels shape is incorrect, please review your code\")\n",
    "\n",
    "if X_test.shape == (61503, 121):\n",
    "    print(\"Success: X_test shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Test dataset shape is incorrect, please review your code\")\n",
    "\n",
    "if isinstance(X_test, pd.DataFrame):\n",
    "    print(\"Success: X_test type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Test dataset type is incorrect, please review your code\")\n",
    "\n",
    "if y_test.shape == (61503,) or y_test.shape == (61503, 1):\n",
    "    print(\"Success: y_test shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Test labels shape is incorrect, please review your code\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46078a39",
   "metadata": {},
   "source": [
    "2.2. In order to avoid overfitting while searching for the best model hyperparameters, it's always a good idea to split our training dataset into two new sets called `train` and `validation`. \n",
    "\n",
    "While the `train` data will be used to fit the model and adjust its internal weights, the `validation` will be exclusively used to test the model performance on unseen data during training, it's like a testing dataset used during experimentation.\n",
    "\n",
    "Remember we can't use the `test` dataset to validate the model performance because this one lacks of labels :( So the `validation` data will be the only resource you will have to evaluate the final model performance before doing your submission.\n",
    "\n",
    "To do that, you will have to complete the function `data_utils.get_train_val_sets()` in all the parts with a `TODO` mark.\n",
    "\n",
    "This function should perform these activities:\n",
    "- Use the `sklearn.model_selection.train_test_split` function with `X_train`, `y_train` datasets.\n",
    "- Assign only 20% of the dataset for testing (see `test_size` parameter in `train_test_split`)\n",
    "- Assign a seed so we get reproducible output across multiple function calls (see `random_state` parameter in `train_test_split`)\n",
    "- Shuffle the data (see `shuffle` parameter in `train_test_split`)\n",
    "\n",
    "For reference, see:\n",
    "- [Scikit-learn train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "- [Wikipedia: Training, validation, and test data sets](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets)\n",
    "- [Train Test Validation Split: How To & Best Practices](https://www.v7labs.com/blog/train-validation-test-set)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec771222",
   "metadata": {},
   "source": [
    "**Don't change anything in this cell, just make it run correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07d31b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: X_train shape is correct!\n",
      "Success: X_train type is correct!\n",
      "Success: y_train shape is correct!\n",
      "Success: X_test shape is correct!\n",
      "Success: X_test type is correct!\n",
      "Success: y_test shape is correct!\n"
     ]
    }
   ],
   "source": [
    "# Now we execute the function above to get the result\n",
    "X_train, X_val, y_train, y_val = data_utils.get_train_val_sets(X_train, y_train)\n",
    "\n",
    "\n",
    "if X_train.shape == (196806, 121):\n",
    "    print(\"Success: X_train shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"X_train dataset shape is incorrect, please review your code\")\n",
    "\n",
    "if isinstance(X_train, pd.DataFrame):\n",
    "    print(\"Success: X_train type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Train dataset type is incorrect, please review your code\")\n",
    "\n",
    "if y_train.shape == (196806,) or y_train.shape == (196806, 1):\n",
    "    print(\"Success: y_train shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Train labels shape is incorrect, please review your code\")\n",
    "\n",
    "if X_val.shape == (49202, 121):\n",
    "    print(\"Success: X_test shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Test dataset shape is incorrect, please review your code\")\n",
    "\n",
    "if isinstance(X_val, pd.DataFrame):\n",
    "    print(\"Success: X_test type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Test dataset type is incorrect, please review your code\")\n",
    "\n",
    "if y_val.shape == (49202,) or y_val.shape == (49202, 1):\n",
    "    print(\"Success: y_test shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Test labels shape is incorrect, please review your code\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51397c38-0204-454b-8fe6-011dc8c38418",
   "metadata": {
    "id": "51397c38-0204-454b-8fe6-011dc8c38418"
   },
   "source": [
    "2.3. In this section, you will code a function to make all the data pre-processing for the dataset. What you have to deliver is a function that takes `X_train`, `X_val`, and `X_test` dataframes, processes all features, and returns the transformed data as numpy arrays ready to be used for training.\n",
    "\n",
    "The function should perform these activities, in this order:\n",
    "\n",
    "1. Correct outliers/anomalous values in numerical columns (`DAYS_EMPLOYED` column)\n",
    "2. Encode string categorical features (dytpe `object`):\n",
    "    - If the feature has 2 categories encode using binary encoding\n",
    "    - More than 2 categories, use one hot encoding \n",
    "3. Impute values for all columns with missing data (use median as imputing value)\n",
    "4. Feature scaling with Min-Max scaler.\n",
    "\n",
    "Complete the function `preprocessing.preprocess_data()` following the instructions given above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "-1YXG39vc2qX",
   "metadata": {
    "id": "-1YXG39vc2qX"
   },
   "source": [
    "In the following cell, we are going to execute the preprocessing function you've just coded. No need to modify this.\n",
    "\n",
    "**Important Note:** From now on, you must always use `train_data` for training your models and `val_data` only for the final evaluation of the model trained. About `test_data`, it will be used at the end only for submitting your final model predictions and be evaluated on our side with the hidden annotations.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "mMkqwi0gd7a8",
   "metadata": {
    "id": "mMkqwi0gd7a8"
   },
   "source": [
    "**Don't change anything in this cell, just make it run correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "whWcb5jtcyYe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1670195395495,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "whWcb5jtcyYe",
    "outputId": "5bb07f0e-a0b6-4773-94b1-97c5f2ccd053"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input train data shape:  (196806, 121)\n",
      "Input val data shape:  (49202, 121)\n",
      "Input test data shape:  (61503, 121) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_train_df[train_columns] = transformed_train_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_val_df[train_columns] = transformed_val_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n",
      "c:\\Users\\Rodrigo Pardo\\Desktop\\PMP\\sprint 02\\assignment\\src\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  working_test_df[train_columns] = transformed_test_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: train_data shape is correct!\n",
      "Success: train_data type is correct!\n",
      "Success: val_data shape is correct!\n",
      "Success: val_data type is correct!\n",
      "Success: test_data shape is correct!\n",
      "Success: test_data type is correct!\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data = preprocessing.preprocess_data(X_train, X_val, X_test)\n",
    "\n",
    "if train_data.shape == (196806, 246):\n",
    "\n",
    "    print(\"Success: train_data shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"train_data dataset shape is incorrect, please review your code\")\n",
    "\n",
    "if isinstance(train_data, np.ndarray):\n",
    "    print(\"Success: train_data type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Train dataset type is incorrect, please review your code\")\n",
    "\n",
    "if val_data.shape == (49202, 246):\n",
    "    print(\"Success: val_data shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"val_data dataset shape is incorrect, please review your code\")\n",
    "\n",
    "if isinstance(val_data, np.ndarray):\n",
    "    print(\"Success: val_data type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Validation dataset type is incorrect, please review your code\")\n",
    "\n",
    "if test_data.shape == (61503, 246):\n",
    "    print(\"Success: test_data shape is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"test_data dataset shape is incorrect, please review your code\")\n",
    "\n",
    "if isinstance(test_data, np.ndarray):\n",
    "    print(\"Success: test_data type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"Test dataset type is incorrect, please review your code\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f5df1e8-1f03-4f14-9dbd-292d3b84859d",
   "metadata": {
    "id": "2f5df1e8-1f03-4f14-9dbd-292d3b84859d"
   },
   "source": [
    "## 3. Training Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83062f02-7157-4083-b57b-78fbc95fe39b",
   "metadata": {
    "id": "83062f02-7157-4083-b57b-78fbc95fe39b"
   },
   "source": [
    "As usual, you will start training simple models and will progressively move to more complex models and pipelines.\n",
    "\n",
    "**Pro tip:** It is of utmost importance to make an accurate estimation of the time required to train a machine learning model. Because of this, we recommend you to use Python [time](https://docs.python.org/3/library/time.html) library or Jupyter magic function `%%time` on the cell you're training your model to get an estimate of the time it took to fit your data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e99a786-a17e-4caa-9ac7-48fced40fa71",
   "metadata": {
    "id": "7e99a786-a17e-4caa-9ac7-48fced40fa71"
   },
   "source": [
    "### Baseline: LogisticRegression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91e10796-6d86-4fce-8738-4b9c4fbbf359",
   "metadata": {
    "id": "91e10796-6d86-4fce-8738-4b9c4fbbf359"
   },
   "source": [
    "3.1. Import LogisticRegression from sklearn and train a model using the preprocesed train data from the previous section, and just default parameters. If you receive a warning because the algorithm failed to converge, try increasing the number of iterations or decreasing the C parameter.\n",
    "\n",
    "Assign the trained model to `log_reg` variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2OTXvqibhqid",
   "metadata": {
    "id": "2OTXvqibhqid"
   },
   "source": [
    "**Don't change anything in this cell, just make it run correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef64563b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.55 s\n",
      "Wall time: 1.27 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.0001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.0001)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.0001)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This is an example code on how to:\n",
    "#   - Create and fit (train) a logistic regression\n",
    "#   - Assign to `log_reg` variable\n",
    "log_reg = None\n",
    "log_reg = LogisticRegression(C=0.0001)\n",
    "log_reg.fit(train_data, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2OTXvqibhqid",
   "metadata": {
    "id": "2OTXvqibhqid"
   },
   "source": [
    "**Don't change anything in this cell, just make it run correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "JmFsb5DShqid",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1670195424262,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "JmFsb5DShqid",
    "outputId": "3cc332c0-18c0-4257-82b7-3e95e05bc012"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Logistic regression model type is correct!\n"
     ]
    }
   ],
   "source": [
    "if isinstance(log_reg, LogisticRegression):\n",
    "    print(\"Success: Logistic regression model type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"Logistic regression model type is incorrect, please review your code\"\n",
    "    )\n",
    "\n",
    "check_is_fitted(log_reg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "018d5c7e-012f-47cc-8bfc-0332de907ae9",
   "metadata": {
    "id": "018d5c7e-012f-47cc-8bfc-0332de907ae9"
   },
   "source": [
    "3.2. Use the trained model to predict probabilities for `train_data` and `val_data`.\n",
    "\n",
    "**Important note:** When using the function `predict_proba()` for getting model probabilities you will get, for each sample, a tuple indicating the probability for class 0 and for class 1 respectively. For computing the AUC ROC score we only need the probability that the debt is not repaid (equivalent to class 1). As an example, the result from running `predict_proba()` on validation dataset will have a shape of `(49202, 2)` but, we only need the second column from that matrix, which corresponds to the class 1.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2OTXvqibhqid",
   "metadata": {
    "id": "2OTXvqibhqid"
   },
   "source": [
    "**Don't change anything in this cell, just make it run correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "798ae69b-1fe2-458d-9de7-3e1ed70b9b02",
   "metadata": {
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1670195428153,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "798ae69b-1fe2-458d-9de7-3e1ed70b9b02"
   },
   "outputs": [],
   "source": [
    "# Example code to show you how to use the Logistic Regression model\n",
    "# to predict probabilities for each class and then, use the probabilities for the\n",
    "# class 1 only.\n",
    "\n",
    "# Train data predictions (class 1)\n",
    "log_reg_train = log_reg.predict_proba(train_data)[:, 1]\n",
    "\n",
    "# Validation data predictions (class 1)\n",
    "log_reg_val = log_reg.predict_proba(val_data)[:, 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0826305-4900-4ba1-bf25-48205be980c6",
   "metadata": {
    "id": "b0826305-4900-4ba1-bf25-48205be980c6"
   },
   "source": [
    "3.3. Get AUC ROC score on train and validation datasets. See [scikit-learn AUC ROC function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) for a working implementation.\n",
    "\n",
    "Assign the AUC ROC score to `lr_roc_auc` variable.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2OTXvqibhqid",
   "metadata": {
    "id": "2OTXvqibhqid"
   },
   "source": [
    "**Don't change anything in this cell, just make it run correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83a850c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC Score: 0.6798\n",
      "Validation ROC AUC Score: 0.6772\n"
     ]
    }
   ],
   "source": [
    "# Example code to show you how to get the ROC AUC Score on train and val datasets\n",
    "\n",
    "# Train ROC AUC Score\n",
    "roc_auc_train = roc_auc_score(y_true=y_train, y_score=log_reg_train)\n",
    "print(f\"Train ROC AUC Score: {roc_auc_train:.4f}\")\n",
    "\n",
    "# Validation ROC AUC Score\n",
    "roc_auc_val = roc_auc_score(y_true=y_val, y_score=log_reg_val)\n",
    "print(f\"Validation ROC AUC Score: {roc_auc_val:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d889582-0c21-4789-acac-4d58f8eb74d9",
   "metadata": {
    "id": "4d889582-0c21-4789-acac-4d58f8eb74d9"
   },
   "source": [
    "At this point, the model should produce a result of around 0.67.\n",
    "\n",
    "**Question:** Comparing train and validation results, do you observe underfitting, overfitting, or none of those two?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ap_JpPoNidg2",
   "metadata": {
    "id": "ap_JpPoNidg2"
   },
   "source": [
    "**Don't change anything in this cell, just make it run correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "KWSHN2Ouidg2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 624,
     "status": "ok",
     "timestamp": 1670195435233,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "KWSHN2Ouidg2",
    "outputId": "03f1bf8e-8980-4eba-d9e8-aa53c5dccd84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: AUC ROC score type is correct!\n",
      "Success: AUC ROC score is correct!\n"
     ]
    }
   ],
   "source": [
    "if isinstance(roc_auc_val, float):\n",
    "    print(\"Success: AUC ROC score type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"AUC ROC score type is incorrect, please review your code\")\n",
    "\n",
    "if roc_auc_val >= 0.6:\n",
    "    print(\"Success: AUC ROC score is correct!\")\n",
    "else:\n",
    "    raise ValueError(\"AUC ROC score is incorrect, please review your code\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f8300cc-d8be-4303-9042-757cb9e15d3e",
   "metadata": {
    "id": "8f8300cc-d8be-4303-9042-757cb9e15d3e"
   },
   "source": [
    "### Training a Random Forest Classifier "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05280b49-11af-4fe6-9236-95f31fb5e0d5",
   "metadata": {
    "id": "05280b49-11af-4fe6-9236-95f31fb5e0d5"
   },
   "source": [
    "You're gonna start working in more complex models: ensambles, particularly, you're going to use the Random Forest Classifier from Scikit Learn. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0efdfd67-ec68-49b3-8727-7ab9784b5e54",
   "metadata": {
    "id": "0efdfd67-ec68-49b3-8727-7ab9784b5e54"
   },
   "source": [
    "3.4. Train a RandomForestClassifier, print the time taken by the fit function. Just use default hyperparameters, except for `n_jobs`, which should be set to \"-1\" to allow the library to use all CPU cores to speed up training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d3b9ea6-4bf9-42f0-aed1-1c0f3a4f9b39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95269,
     "status": "ok",
     "timestamp": 1670195539807,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "1d3b9ea6-4bf9-42f0-aed1-1c0f3a4f9b39",
    "outputId": "848010ac-1f2b-43ab-9873-0d78c186e804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 29s\n",
      "Wall time: 1min 33s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TODO Write your code here for training a Random Forest model.\n",
    "#   - Please use sklearn.ensemble.RandomForestClassifier() class.\n",
    "#   - Assign the model to the variable `rf`.\n",
    "#   - Remember to fit the model only on `train_data`.\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(train_data, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c70bf912",
   "metadata": {
    "id": "2OTXvqibhqid"
   },
   "source": [
    "**Don't change anything in this cell, just make it run correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a21b91c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1670195424262,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "JmFsb5DShqid",
    "outputId": "3cc332c0-18c0-4257-82b7-3e95e05bc012"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: RandomForestClassifier model type is correct!\n"
     ]
    }
   ],
   "source": [
    "if isinstance(rf, RandomForestClassifier):\n",
    "    print(\"Success: RandomForestClassifier model type is correct!\")\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"RandomForestClassifier model type is incorrect, please review your code\"\n",
    "    )\n",
    "\n",
    "check_is_fitted(rf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b12a0a5-8ff1-4cb6-9928-37122d2a3435",
   "metadata": {
    "id": "3b12a0a5-8ff1-4cb6-9928-37122d2a3435"
   },
   "source": [
    "3.5. Use the classifier to predict probabilities for `train_data` and `val_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ba80108-858c-4d4e-ba19-7f19fa526dc9",
   "metadata": {
    "executionInfo": {
     "elapsed": 4992,
     "status": "ok",
     "timestamp": 1670195577761,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "4ba80108-858c-4d4e-ba19-7f19fa526dc9"
   },
   "outputs": [],
   "source": [
    "# TODO Use the Random Forest model to predict probabilities for each class and then,\n",
    "# use the probabilities for the class 1 only.\n",
    "\n",
    "# Train data predictions (class 1)\n",
    "rf_pred_train = rf.predict_proba(train_data)[:, 1]\n",
    "\n",
    "# Validation data predictions (class 1)\n",
    "rf_pred_val = rf.predict_proba(val_data)[:, 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "W3TrmlUF2pDM",
   "metadata": {
    "id": "W3TrmlUF2pDM"
   },
   "source": [
    "3.6. Get AUC ROC score on train and validation datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "D4jft3Sw2pDN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1670195702088,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "D4jft3Sw2pDN",
    "outputId": "74877c11-92b3-43d3-afbe-08fb51201708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC Score:  1.0\n",
      "Validation ROC AUC Score:  0.7098742780921491\n"
     ]
    }
   ],
   "source": [
    "# TODO Get the ROC AUC Score on train_data and val_data datasets.\n",
    "# Train ROC AUC Score\n",
    "roc_auc_train = roc_auc_score(y_true=y_train, y_score=rf_pred_train)\n",
    "\n",
    "# Validation ROC AUC Score\n",
    "roc_auc_val = roc_auc_score(y_true=y_val, y_score=rf_pred_val)\n",
    "\n",
    "print('Train ROC AUC Score: ', roc_auc_train)\n",
    "print('Validation ROC AUC Score: ', roc_auc_val)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8_buAhkG24ZC",
   "metadata": {
    "id": "8_buAhkG24ZC"
   },
   "source": [
    "At this point, the model should produce a result around 0.7.\n",
    "\n",
    "**Question:** Comparing train and validation results, do you observe underfitting, overfitting, or none of those two?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4acfcc24-62b6-4118-9c42-a0268dcf5c53",
   "metadata": {
    "id": "4acfcc24-62b6-4118-9c42-a0268dcf5c53"
   },
   "source": [
    "### Randomized Search with Cross Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd634b2b-68af-4db1-b062-1496f8d1179e",
   "metadata": {
    "id": "dd634b2b-68af-4db1-b062-1496f8d1179e"
   },
   "source": [
    "So far, we've only created models using the default hyperparameters of each algorithm. This is usually something that we would only do for baseline models, hyperparameter tuning is a very important part of the modeling process and is often the difference between having an acceptable model or not.\n",
    "\n",
    "But, there are usually lots of hyperparameters to tune and a finite amount of time to do it, you have to consider the time and resources it takes to find an optimal combination of them. In the previous section you trained a random forest classifier and saw how much it took to train it once in your PC. If you want to do hyperparameter optimization you now have to consider that you will have to train the algorithm N number of times, with N being the cartesian product of all parameters. \n",
    "\n",
    "Furthermore, you can't validate the performance of your trained models on the test set, as this data should only be used to validate the final model. So we have to implement a validation strategy, K-Fold Cross Validation being the most common. But this also adds time complexity to our training, because we will have to train each combinations of hyperparameters M number of times, X being the number of folds in which we divided our dataset, so the total number of training iterations will be NxM... this resulting number can grow VERY quickly.\n",
    "\n",
    "Fortunately there are strategies to mitigate this, here you're going to select a small number of hyperparameters to test a RandomForestClassifier, and use a Randomized Search algorithm with K-Fold Cross Validation to avoid doing a full search across the grid. \n",
    "\n",
    "Remember: take in consideration how much time it took to train a single classifier, and define the number of cross validations folds and iterations of the search accordingly. \n",
    "A recommendation: run the training process, go make yourself a cup of coffee, sit somewhere comfortably and forget about it for a while.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c49a11c7-156f-46bb-8bba-be29d1b2ed1a",
   "metadata": {
    "id": "c49a11c7-156f-46bb-8bba-be29d1b2ed1a"
   },
   "source": [
    "3.7. Use `sklearn.model_selection.RandomizedSearchCV()` to find the best combination of hyperparameters for a Random Forest model. \n",
    "\n",
    "The validation metric used to evaluate the models should be \"roc_auc\" (i.e. `scoring=\"roc_auc\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO Write your code here for training a Random Forest model using Random Search\n",
    "# of hyper-parameters.\n",
    "#   - Please use sklearn.model_selection.RandomizedSearchCV() and\n",
    "#     sklearn.ensemble.RandomForestClassifier() classes.\n",
    "#   - Assign the RandomizedSearchCV model to the variable `rf_random`.\n",
    "#   - Remember to fit the model only on `train_data`.\n",
    "rf_random = \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b09205aa-5afc-481a-b45b-d5e80b56d804",
   "metadata": {
    "id": "b09205aa-5afc-481a-b45b-d5e80b56d804"
   },
   "source": [
    "3.8. Use the classifier to predict probabilities on the train and test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4429ca94-57f4-487e-b3dd-883bc7bc7835",
   "metadata": {
    "executionInfo": {
     "elapsed": 10144,
     "status": "ok",
     "timestamp": 1670198429448,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "4429ca94-57f4-487e-b3dd-883bc7bc7835"
   },
   "outputs": [],
   "source": [
    "# TODO Use the RandomizedSearchCV model to predict probabilities for each class and\n",
    "# then, use the probabilities for the class 1 only.\n",
    "\n",
    "# Train data predictions (class 1)\n",
    "rf_tuned_pred_train =\n",
    "\n",
    "# Validation data predictions (class 1)\n",
    "rf_tuned_pred_val ="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e645e849-a7e0-43af-b7d1-0b84c29f0e70",
   "metadata": {
    "id": "e645e849-a7e0-43af-b7d1-0b84c29f0e70"
   },
   "source": [
    "3.9. Get AUC ROC score on train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f2204-304a-405b-b51b-350f054eb3f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 412,
     "status": "ok",
     "timestamp": 1670199416165,
     "user": {
      "displayName": "Jose Luis",
      "userId": "17952480099147442429"
     },
     "user_tz": 180
    },
    "id": "953f2204-304a-405b-b51b-350f054eb3f4",
    "outputId": "e0c4bfdd-1188-4e6f-9bee-0148aa036543"
   },
   "outputs": [],
   "source": [
    "# TODO Get the ROC AUC Score on train_data and val_data datasets.\n",
    "# Train ROC AUC Score\n",
    "roc_auc_train =\n",
    "\n",
    "# Validation ROC AUC Score\n",
    "roc_auc_val ="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "_ZAAkhx1X1Qt",
   "metadata": {
    "id": "_ZAAkhx1X1Qt"
   },
   "source": [
    "At this point, the model should produce a result around 0.7 or higher.\n",
    "\n",
    "**Question:** Comparing train and validation results, do you observe underfitting, overfitting, or none of those two?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2143f7b6",
   "metadata": {},
   "source": [
    "## 4. Predict unlabeled data\n",
    "\n",
    "Now it's time to finally use the `test_data` samples. Because we don't have the labels we can't see how the model performs on this dataset (╯°□°)╯︵ ┻━┻\n",
    "\n",
    "But... don't worry, we will internally evaluate your model and give feedback on the results!\n",
    "\n",
    "In the cells below:\n",
    "- Take your best model\n",
    "- Take `test_data` (i.e. the dataset after doing the preprocessing and feature engineering part)\n",
    "- Run the data through your model and save the predictions on the `TARGET` column in the `app_test` DataFrame (yeah that we've loaded at the very beginning of this notebook).\n",
    "    - `TARGET` column values must be the probabilities for class 1. So remember to use the `predict_proba()` function from your model as we did in the previous sections.\n",
    "- Save the modified version of the DataFrame with the same name it has before (`dataset/application_test_aai.csv`) and don't forget to submit it alongside the rest of this sprint project code\n",
    "- And finally, don't get confused, you shouldn't submit `dataset/application_train_aai.csv`. So please don't upload your solution with this heavy dataset inside.\n",
    "\n",
    "Let's say your best model is called `best_credit_model_ever`, then your code should be exactly this:\n",
    "\n",
    "```python\n",
    "    test_preds = best_credit_model_ever.predict_proba(test_data)[:, 1]\n",
    "    app_test[\"TARGET\"] = test_preds\n",
    "    app_test.to_csv(config.DATASET_TEST, index=False)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99b20f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO Use your best model and call the predict_proba() on test_data then,\n",
    "# use the probabilities for the class 1 only.\n",
    "# Then, put the predictions in app_test[\"TARGET\"] and save the DataFrame as a csv\n",
    "# with the same name it originally has (\"application_test_aai.csv\").\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sp02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "e94ba2b62eb694bae49fa8de0ed9e62de168312db56a1c4a0a8d614a40cedec1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
